# R√©seau de Neurones pour la Reconnaissance de Chiffres MNIST

Ce projet pr√©sente l'impl√©mentation d'un r√©seau de neurones simple √† partir de z√©ro en utilisant uniquement la biblioth√®que **NumPy**. L'objectif est de reconna√Ætre des chiffres manuscrits provenant du c√©l√®bre dataset MNIST.

Ce travail a √©t√© r√©alis√© dans le but de comprendre en profondeur les m√©canismes internes d'un r√©seau de neurones, notamment la **propagation avant (forward propagation)**, la **r√©tropropagation (backpropagation)** et la **descente de gradient (gradient descent)**.

## üöÄ Fonctionnalit√©s

- **Aucune biblioth√®que de haut niveau** : Le r√©seau est construit sans TensorFlow, PyTorch ou Scikit-learn.
- **Architecture simple** : Un r√©seau avec une couche d'entr√©e, une couche cach√©e et une couche de sortie.
- **Fonctions d'activation** : Utilisation de **ReLU** pour la couche cach√©e et **Softmax** pour la couche de sortie.
- **Pr√©paration des donn√©es** : Un notebook est inclus pour charger le dataset MNIST original et le convertir au format CSV.
- **Entra√Ænement et √âvaluation** : Le mod√®le est entra√Æn√© et sa performance est √©valu√©e sur un ensemble de validation distinct.

## üõ†Ô∏è Comment l'utiliser

### Pr√©requis

- Python 3.x
- Les biblioth√®ques list√©es dans `requirements.txt`

### Installation

1.  Clonez ce d√©p√¥t :
    ```bash
    git clone https://github.com/Linwe-e/Neural-Network-MNIST-Numpy-.git
    ```
2.  Naviguez dans le r√©pertoire du projet :
    ```bash
    cd Neural-Network-MNIST-Numpy-
    ```
3.  Installez les d√©pendances :
    ```bash
    pip install -r requirements.txt
    ```

### Ex√©cution

1.  **Pr√©paration des donn√©es** :
    - Ouvrez et ex√©cutez le notebook `ReadMNISTDataset.ipynb`. Cela g√©n√©rera les fichiers `mnist_train_full.csv` et `mnist_test_full.csv`.

2.  **Entra√Ænement du mod√®le** :
    - Ouvrez et ex√©cutez le notebook `MNISTDigitRecognition.ipynb`. Vous pouvez y ajuster les hyperparam√®tres (nombre de neurones, taux d'apprentissage, nombre d'it√©rations) et observer l'entra√Ænement en temps r√©el.

## üß† Concepts impl√©ment√©s

- **Initialisation des poids et des biais**
- **Forward Propagation**
- **Calcul de la fonction de co√ªt (Cross-Entropy)**
- **Backpropagation**
- **Mise √† jour des param√®tres via la descente de gradient**

## üìä Exp√©rimentations et R√©sultats

Plusieurs configurations d'hyperparam√®tres ont √©t√© test√©es pour optimiser la performance du mod√®le. Voici un r√©sum√© des exp√©rimentations :

| Couche Cach√©e | Taux d'apprentissage | It√©rations | Pr√©cision (Validation) |
| :-----------: | :------------------: | :--------: | :--------------------: |
| 10 neurones   | 0.1                  | 500        | ~85%                   |
| 10 neurones   | 0.01                 | 700        | ~73% (sous-apprentissage) |
| **28 neurones**   | **0.05**             | **2000**   | **~91.1%**             |

La meilleure configuration obtenue atteint **91.10%** de pr√©cision sur l'ensemble de validation.

---

Ce projet est une excellente base pour quiconque souhaite apprendre les fondements des r√©seaux de neurones de mani√®re pratique.
