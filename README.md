# R√©seau de Neurones pour la Reconnaissance de Chiffres MNIST

![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-Only-orange?style=for-the-badge&logo=numpy&logoColor=white)
![MNIST](https://img.shields.io/badge/Dataset-MNIST-green?style=for-the-badge)
![Accuracy](https://img.shields.io/badge/Accuracy-91.1%25-brightgreen?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)

> üá´üá∑ **Version fran√ßaise** | üá∫üá∏ **[English version below](#english-version)** (voir plus bas)

Ce projet pr√©sente l'impl√©mentation d'un r√©seau de neurones simple √† partir de z√©ro en utilisant uniquement la biblioth√®que **NumPy**. L'objectif est de reconna√Ætre des chiffres manuscrits provenant du c√©l√®bre dataset MNIST.

Ce travail a √©t√© r√©alis√© dans le but de comprendre en profondeur les m√©canismes internes d'un r√©seau de neurones, notamment la **propagation avant (forward propagation)**, la **r√©tropropagation (backpropagation)** et la **descente de gradient (gradient descent)**.

## üöÄ Fonctionnalit√©s

![No Dependencies](https://img.shields.io/badge/Dependencies-Minimal-lightblue?style=flat-square)
![From Scratch](https://img.shields.io/badge/Implementation-From%20Scratch-purple?style=flat-square)
![Neural Network](https://img.shields.io/badge/Architecture-Simple%20NN-red?style=flat-square)

- **Aucune biblioth√®que de haut niveau** : Le r√©seau est construit sans TensorFlow, PyTorch ou Scikit-learn.
- **Architecture simple** : Un r√©seau avec une couche d'entr√©e, une couche cach√©e et une couche de sortie.
- **Fonctions d'activation** : Utilisation de **ReLU** pour la couche cach√©e et **Softmax** pour la couche de sortie.
- **Pr√©paration des donn√©es** : Un notebook est inclus pour charger le dataset MNIST original et le convertir au format CSV.
- **Entra√Ænement et √âvaluation** : Le mod√®le est entra√Æn√© et sa performance est √©valu√©e sur un ensemble de validation distinct.

## üõ†Ô∏è Comment l'utiliser

### Pr√©requis

- Python 3.x
- Les biblioth√®ques list√©es dans `requirements.txt`

### Installation

1.  Clonez ce d√©p√¥t :
    ```bash
    git clone https://github.com/Linwe-e/Neural-Network-MNIST-Numpy-.git
    ```
2.  Naviguez dans le r√©pertoire du projet :
    ```bash
    cd Neural-Network-MNIST-Numpy-
    ```
3.  Installez les d√©pendances :
    ```bash
    pip install -r requirements.txt
    ```

### Ex√©cution

1.  **Pr√©paration des donn√©es** :
    - Ouvrez et ex√©cutez le notebook `ReadMNISTDataset.ipynb`. Cela g√©n√©rera les fichiers `mnist_train_full.csv` et `mnist_test_full.csv`.

2.  **Entra√Ænement du mod√®le** :
    - Ouvrez et ex√©cutez le notebook `MNISTDigitRecognition.ipynb`. Vous pouvez y ajuster les hyperparam√®tres (nombre de neurones, taux d'apprentissage, nombre d'it√©rations) et observer l'entra√Ænement en temps r√©el.

## üß† Concepts impl√©ment√©s

- **Initialisation des poids et des biais**
- **Forward Propagation**
- **Calcul de la fonction de co√ªt (Cross-Entropy)**
- **Backpropagation**
- **Mise √† jour des param√®tres via la descente de gradient**

## üìä Exp√©rimentations et R√©sultats

![Best Config](https://img.shields.io/badge/Best%20Config-28%20neurons-success?style=flat-square)
![Learning Rate](https://img.shields.io/badge/Learning%20Rate-0.05-informational?style=flat-square)
![Iterations](https://img.shields.io/badge/Iterations-2000-blueviolet?style=flat-square)

Plusieurs configurations d'hyperparam√®tres ont √©t√© test√©es pour optimiser la performance du mod√®le. Voici un r√©sum√© des exp√©rimentations :

| Couche Cach√©e | Taux d'apprentissage | It√©rations | Pr√©cision (Validation) |
| :-----------: | :------------------: | :--------: | :--------------------: |
| 10 neurones   | 0.1                  | 500        | ~85%                   |
| 10 neurones   | 0.01                 | 700        | ~73% (sous-apprentissage) |
| **28 neurones**   | **0.05**             | **2000**   | **~91.1%**             |

La meilleure configuration obtenue atteint **91.10%** de pr√©cision sur l'ensemble de validation.

---

Ce projet est une excellente base pour quiconque souhaite apprendre les fondements des r√©seaux de neurones de mani√®re pratique.

---

# üá∫üá∏ English Version

# Neural Network for MNIST Digit Recognition

![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-Only-orange?style=for-the-badge&logo=numpy&logoColor=white)
![MNIST](https://img.shields.io/badge/Dataset-MNIST-green?style=for-the-badge)
![Accuracy](https://img.shields.io/badge/Accuracy-91.1%25-brightgreen?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)

> üá∫üá∏ **English version** | üá´üá∑ **[Version fran√ßaise ci-dessus](#r√©seau-de-neurones-pour-la-reconnaissance-de-chiffres-mnist)** (see above)

This project presents the implementation of a simple neural network from scratch using only the **NumPy** library. The goal is to recognize handwritten digits from the famous MNIST dataset.

This work was carried out to understand in depth the internal mechanisms of a neural network, particularly **forward propagation**, **backpropagation** and **gradient descent**.

## üöÄ Features

![No Dependencies](https://img.shields.io/badge/Dependencies-Minimal-lightblue?style=flat-square)
![From Scratch](https://img.shields.io/badge/Implementation-From%20Scratch-purple?style=flat-square)
![Neural Network](https://img.shields.io/badge/Architecture-Simple%20NN-red?style=flat-square)

- **No high-level libraries**: The network is built without TensorFlow, PyTorch or Scikit-learn.
- **Simple architecture**: A network with an input layer, a hidden layer and an output layer.
- **Activation functions**: Use of **ReLU** for the hidden layer and **Softmax** for the output layer.
- **Data preparation**: A notebook is included to load the original MNIST dataset and convert it to CSV format.
- **Training and Evaluation**: The model is trained and its performance is evaluated on a separate validation set.

## üõ†Ô∏è How to use

### Prerequisites

- Python 3.x
- The libraries listed in `requirements.txt`

### Installation

1.  Clone this repository:
    ```bash
    git clone https://github.com/Linwe-e/Neural-Network-MNIST-Numpy-.git
    ```
2.  Navigate to the project directory:
    ```bash
    cd Neural-Network-MNIST-Numpy-
    ```
3.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### Execution

1.  **Data preparation**:
    - Open and run the `ReadMNISTDataset.ipynb` notebook. This will generate the `mnist_train_full.csv` and `mnist_test_full.csv` files.

2.  **Model training**:
    - Open and run the `MNISTDigitRecognition.ipynb` notebook. You can adjust the hyperparameters (number of neurons, learning rate, number of iterations) and observe the training in real time.

## üß† Implemented concepts

- **Weight and bias initialization**
- **Forward Propagation**
- **Cost function calculation (Cross-Entropy)**
- **Backpropagation**
- **Parameter update via gradient descent**

## üìä Experiments and Results

![Best Config](https://img.shields.io/badge/Best%20Config-28%20neurons-success?style=flat-square)
![Learning Rate](https://img.shields.io/badge/Learning%20Rate-0.05-informational?style=flat-square)
![Iterations](https://img.shields.io/badge/Iterations-2000-blueviolet?style=flat-square)

Several hyperparameter configurations have been tested to optimize model performance. Here is a summary of the experiments:

| Hidden Layer | Learning Rate | Iterations | Accuracy (Validation) |
| :----------: | :-----------: | :--------: | :-------------------: |
| 10 neurons   | 0.1           | 500        | ~85%                  |
| 10 neurons   | 0.01          | 700        | ~73% (underfitting)   |
| **28 neurons**   | **0.05**      | **2000**   | **~91.1%**           |

The best configuration achieved **91.10%** accuracy on the validation set.

---

This project is an excellent foundation for anyone who wants to learn the fundamentals of neural networks in a practical way.
