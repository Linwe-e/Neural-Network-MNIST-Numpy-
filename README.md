<a name="version-francaise"></a>
# RÃ©seau de Neurones pour la Reconnaissance de Chiffres MNIST

![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-Only-orange?style=for-the-badge&logo=numpy&logoColor=white)
![MNIST](https://img.shields.io/badge/Dataset-MNIST-green?style=for-the-badge)
![Accuracy](https://img.shields.io/badge/Accuracy-91.1%25-brightgreen?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)

> ğŸ‡«ğŸ‡· **Version franÃ§aise** | ğŸ‡ºğŸ‡¸ **[English version below](#english-version)** (voir plus bas)

Ce projet prÃ©sente l'implÃ©mentation d'un rÃ©seau de neurones simple Ã  partir de zÃ©ro en utilisant uniquement la bibliothÃ¨que **NumPy**. L'objectif est de reconnaÃ®tre des chiffres manuscrits provenant du cÃ©lÃ¨bre dataset MNIST.

Ce travail a Ã©tÃ© rÃ©alisÃ© dans le but de comprendre en profondeur les mÃ©canismes internes d'un rÃ©seau de neurones, notamment la **propagation avant (forward propagation)**, la **rÃ©tropropagation (backpropagation)** et la **descente de gradient (gradient descent)**.

## ğŸš€ FonctionnalitÃ©s

![No Dependencies](https://img.shields.io/badge/Dependencies-Minimal-lightblue?style=flat-square)
![From Scratch](https://img.shields.io/badge/Implementation-From%20Scratch-purple?style=flat-square)
![Neural Network](https://img.shields.io/badge/Architecture-Simple%20NN-red?style=flat-square)

- **Aucune bibliothÃ¨que de haut niveau** : Le rÃ©seau est construit sans TensorFlow, PyTorch ou Scikit-learn.
- **Architecture simple** : Un rÃ©seau avec une couche d'entrÃ©e, une couche cachÃ©e et une couche de sortie.
- **Fonctions d'activation** : Utilisation de **ReLU** pour la couche cachÃ©e et **Softmax** pour la couche de sortie.
- **PrÃ©paration des donnÃ©es** : Un notebook est inclus pour charger le dataset MNIST original et le convertir au format CSV.
- **EntraÃ®nement et Ã‰valuation** : Le modÃ¨le est entraÃ®nÃ© et sa performance est Ã©valuÃ©e sur un ensemble de validation distinct.

## ğŸ› ï¸ Comment l'utiliser

### PrÃ©requis

- Python 3.x
- Les bibliothÃ¨ques listÃ©es dans `requirements.txt`

### Installation

1.  Clonez ce dÃ©pÃ´t :
    ```bash
    git clone https://github.com/Linwe-e/Neural-Network-MNIST-Numpy-.git
    ```
2.  Naviguez dans le rÃ©pertoire du projet :
    ```bash
    cd Neural-Network-MNIST-Numpy-
    ```
3.  Installez les dÃ©pendances :
    ```bash
    pip install -r requirements.txt
    ```

### ExÃ©cution

1.  **PrÃ©paration des donnÃ©es** :
    - Ouvrez et exÃ©cutez le notebook `ReadMNISTDataset.ipynb`. Cela gÃ©nÃ©rera les fichiers `mnist_train_full.csv` et `mnist_test_full.csv`.

2.  **EntraÃ®nement du modÃ¨le** :
    - Ouvrez et exÃ©cutez le notebook `MNISTDigitRecognition.ipynb`. Vous pouvez y ajuster les hyperparamÃ¨tres (nombre de neurones, taux d'apprentissage, nombre d'itÃ©rations) et observer l'entraÃ®nement en temps rÃ©el.

## ğŸ§  Concepts implÃ©mentÃ©s

- **Initialisation des poids et des biais**
- **Forward Propagation**
- **Calcul de la fonction de coÃ»t (Cross-Entropy)**
- **Backpropagation**
- **Mise Ã  jour des paramÃ¨tres via la descente de gradient**

## ğŸ“Š ExpÃ©rimentations et RÃ©sultats

![Best Config](https://img.shields.io/badge/Best%20Config-28%20neurons-success?style=flat-square)
![Learning Rate](https://img.shields.io/badge/Learning%20Rate-0.05-informational?style=flat-square)
![Iterations](https://img.shields.io/badge/Iterations-2000-blueviolet?style=flat-square)

Plusieurs configurations d'hyperparamÃ¨tres ont Ã©tÃ© testÃ©es pour optimiser la performance du modÃ¨le. Voici un rÃ©sumÃ© des expÃ©rimentations :

| Couche CachÃ©e | Taux d'apprentissage | ItÃ©rations | PrÃ©cision (Validation) |
| :-----------: | :------------------: | :--------: | :--------------------: |
| 10 neurones   | 0.1                  | 500        | ~85%                   |
| 10 neurones   | 0.01                 | 700        | ~73% (sous-apprentissage) |
| **28 neurones**   | **0.05**             | **2000**   | **~91.1%**             |

La meilleure configuration obtenue atteint **91.10%** de prÃ©cision sur l'ensemble de validation.

## ğŸ“š Ressources et RÃ©fÃ©rences

Ce projet s'inspire et utilise les ressources suivantes :

- **ğŸ“„ Article Medium** : [Building a Simple Neural Network from Scratch for MNIST Digit Recognition](https://medium.com/@ombaval/building-a-simple-neural-network-from-scratch-for-mnist-digit-recognition-without-using-7005a7733418)
- **ğŸ—‚ï¸ Dataset MNIST** : [MNIST Original Dataset sur Kaggle](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)
- **ğŸ““ Notebook de lecture MNIST** : [Read MNIST Dataset Notebook](https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook)

---

Ce projet est une excellente base pour quiconque souhaite apprendre les fondements des rÃ©seaux de neurones de maniÃ¨re pratique.

---

<a name="english-version"></a>
# ğŸ‡ºğŸ‡¸ English Version

# Neural Network for MNIST Digit Recognition

![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-Only-orange?style=for-the-badge&logo=numpy&logoColor=white)
![MNIST](https://img.shields.io/badge/Dataset-MNIST-green?style=for-the-badge)
![Accuracy](https://img.shields.io/badge/Accuracy-91.1%25-brightgreen?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)

> ğŸ‡ºğŸ‡¸ **English version** | ğŸ‡«ğŸ‡· **[Version franÃ§aise ci-dessus](#version-francaise)** (see above)

This project presents the implementation of a simple neural network from scratch using only the **NumPy** library. The goal is to recognize handwritten digits from the famous MNIST dataset.

This work was carried out to understand in depth the internal mechanisms of a neural network, particularly **forward propagation**, **backpropagation** and **gradient descent**.

## ğŸš€ Features

![No Dependencies](https://img.shields.io/badge/Dependencies-Minimal-lightblue?style=flat-square)
![From Scratch](https://img.shields.io/badge/Implementation-From%20Scratch-purple?style=flat-square)
![Neural Network](https://img.shields.io/badge/Architecture-Simple%20NN-red?style=flat-square)

- **No high-level libraries**: The network is built without TensorFlow, PyTorch or Scikit-learn.
- **Simple architecture**: A network with an input layer, a hidden layer and an output layer.
- **Activation functions**: Use of **ReLU** for the hidden layer and **Softmax** for the output layer.
- **Data preparation**: A notebook is included to load the original MNIST dataset and convert it to CSV format.
- **Training and Evaluation**: The model is trained and its performance is evaluated on a separate validation set.

## ğŸ› ï¸ How to use

### Prerequisites

- Python 3.x
- The libraries listed in `requirements.txt`

### Installation

1.  Clone this repository:
    ```bash
    git clone https://github.com/Linwe-e/Neural-Network-MNIST-Numpy-.git
    ```
2.  Navigate to the project directory:
    ```bash
    cd Neural-Network-MNIST-Numpy-
    ```
3.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### Execution

1.  **Data preparation**:
    - Open and run the `ReadMNISTDataset.ipynb` notebook. This will generate the `mnist_train_full.csv` and `mnist_test_full.csv` files.

2.  **Model training**:
    - Open and run the `MNISTDigitRecognition.ipynb` notebook. You can adjust the hyperparameters (number of neurons, learning rate, number of iterations) and observe the training in real time.

## ğŸ§  Implemented concepts

- **Weight and bias initialization**
- **Forward Propagation**
- **Cost function calculation (Cross-Entropy)**
- **Backpropagation**
- **Parameter update via gradient descent**

## ğŸ“Š Experiments and Results

![Best Config](https://img.shields.io/badge/Best%20Config-28%20neurons-success?style=flat-square)
![Learning Rate](https://img.shields.io/badge/Learning%20Rate-0.05-informational?style=flat-square)
![Iterations](https://img.shields.io/badge/Iterations-2000-blueviolet?style=flat-square)

Several hyperparameter configurations have been tested to optimize model performance. Here is a summary of the experiments:

| Hidden Layer | Learning Rate | Iterations | Accuracy (Validation) |
| :----------: | :-----------: | :--------: | :-------------------: |
| 10 neurons   | 0.1           | 500        | ~85%                  |
| 10 neurons   | 0.01          | 700        | ~73% (underfitting)   |
| **28 neurons**   | **0.05**      | **2000**   | **~91.1%**           |

The best configuration achieved **91.10%** accuracy on the validation set.

## ğŸ“š Resources and References

This project is inspired by and uses the following resources:

- **ğŸ“„ Medium Article**: [Building a Simple Neural Network from Scratch for MNIST Digit Recognition](https://medium.com/@ombaval/building-a-simple-neural-network-from-scratch-for-mnist-digit-recognition-without-using-7005a7733418)
- **ğŸ—‚ï¸ MNIST Dataset**: [MNIST Original Dataset on Kaggle](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)
- **ğŸ““ MNIST Reading Notebook**: [Read MNIST Dataset Notebook](https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook)

---

This project is an excellent foundation for anyone who wants to learn the fundamentals of neural networks in a practical way.
